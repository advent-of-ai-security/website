---
title: Door 09 - Misinformation
description: How LLM hallucinations, training data biases, and overreliance create credible-sounding false information that damages trust, safety, and legal compliance.
date: 2025-12-21
meta:
  - Door 09
  - OWASP - LLM09:2025
---
import TLDR from '@/components/ui/TLDR.tsx';
import Quote from '@/components/ui/Quote.tsx';
import Section from '@/components/ui/Section.tsx';
import Link from '@/components/ui/Link.tsx';
import { List, Item } from '@/components/ui/List.tsx';
import { Steps, Step } from '@/components/ui/Steps.tsx';
import OverrelianceLab from '@/components/ui/OverrelianceLab.tsx';

<TLDR>
  <div className="space-y-4">
    <Quote minimal source="OWASP LLM09:2025 Misinformation" href="https://genai.owasp.org/llmrisk/llm092025-misinformation/">
      Misinformation from LLMs poses a core vulnerability for applications relying on these models. LLMs can generate false but credible-sounding information (hallucinations), amplified by user overreliance—leading to security breaches, reputational harm, and legal liability.
    </Quote>
    <List>
      <Item><b>Hallucinations Are Structural.</b> LLMs don't "know" facts—they predict text patterns. When data is missing, they fabricate plausible-sounding answers using statistical patterns, creating authoritative-seeming lies.</Item>
      <Item><b>Overreliance Amplifies Risk.</b> Users trust fluent, confident outputs without verification, integrating false information into decisions, code, medical advice, and legal arguments.</Item>
      <Item><b>Not Just Wrong—Dangerous.</b> Misinformation in safety-critical domains (healthcare, finance, law) causes real harm: wrong diagnoses, financial losses, failed legal cases.</Item>
    </List>
  </div>
</TLDR>

<Section title="What Is Misinformation in LLMs?" meta="DEFINITION">
  <p>
    Misinformation in LLM context refers to <b>confident, coherent, but factually incorrect information</b> generated by the model. Unlike traditional software bugs that crash or return errors, LLMs fail gracefully—they generate plausible lies that humans accept as truth.
  </p>
  <p>
    The root cause is the model's architecture: LLMs are trained to predict the next word, not to verify facts. When asked "Who won the 2024 Super Bowl?" and lacking that data, the model doesn't say "I don't know"—it generates a statistically likely team name based on historical patterns, confidently asserting fiction.
  </p>
  <Quote source="OWASP LLM09:2025" href="https://genai.owasp.org/llmrisk/llm09-overreliance/">
    Hallucination occurs when LLMs fill gaps in their training data using statistical patterns, resulting in answers that sound accurate but are unfounded. Biases in training data and incomplete information amplify this effect.
  </Quote>
</Section>

<Section title="Why It Matters" meta="IMPACT">
  <p>
    Misinformation isn't just an accuracy problem—it's a security, legal, and safety crisis when LLMs are deployed in high-stakes contexts. Recent research reveals misinformation actively changes human beliefs through persuasive deception.
  </p>
  <List>
    <Item>
      <b>Deceptive Explanations Outperform Truth.</b> <Link href="https://dl.acm.org/doi/10.1145/3706598.3713408">CHI 2025 research</Link> with 11,780 observations from 589 participants demonstrates that deceptive AI-generated explanations are <b>more convincing than honest explanations</b>, increasing belief in false news while undermining true information. Even logically invalid deceptive arguments retain persuasive power, revealing a fundamental vulnerability in human-AI interaction.
    </Item>
    <Item>
      <b>AI-Generated Fake News Epidemic.</b> <Link href="https://arxiv.org/abs/2410.19250">2024 research</Link> shows LLMs systematically generate convincing fake news at scale. While LLMs detect real news 68% better than humans, both achieve only ~60% accuracy detecting AI-generated fake content—demonstrating the asymmetric advantage attackers have in creating vs. detecting misinformation.
    </Item>
    <Item>
      <b>Security Breaches via False Guidance.</b> LLMs used in security operations might hallucinate: "CVE-2024-FAKE is patched in version 2.1" when it doesn't exist, causing teams to ignore real vulnerabilities or deploy unpatched systems.
    </Item>
    <Item>
      <b>Legal Liability (Fake Citations).</b> Lawyers using LLMs have submitted briefs citing <b>non-existent court cases</b> that the model hallucinated. Courts sanctioned attorneys for relying on AI-generated false precedents. <Link href="https://www.indusface.com/blog/owasp-top-10-llm/">Multiple documented cases</Link> in 2023-2024.
    </Item>
    <Item>
      <b>Medical Misinformation.</b> Healthcare chatbots hallucinating drug interactions, dosages, or treatment protocols create life-threatening risks. Users trust medical advice delivered in authoritative language without verifying against clinical sources.
    </Item>
    <Item>
      <b>Financial Losses.</b> Investment chatbots generating false market analysis or non-existent financial instruments lead to poor investment decisions and regulatory violations.
    </Item>
    <Item>
      <b>Code Generation Risks.</b> LLMs hallucinating API methods, library functions, or security patterns cause developers to ship vulnerable code. Example: Model invents a "safe" SQL query pattern that's actually injection-vulnerable.
    </Item>
  </List>
</Section>

<Section title="Taxonomy of Misinformation" meta="ATTACK VECTORS">
  <p>
    Misinformation manifests in multiple forms, each with distinct causes and risks.
  </p>
  <List>
    <Item>
      <b>Factual Hallucinations.</b> The model invents facts, statistics, or events that never occurred. Example: "The Eiffel Tower was completed in 1895" (actually 1889) or "Python 4.0 introduces quantum computing primitives" (doesn't exist).
    </Item>
    <Item>
      <b>Citation Fabrication.</b> LLMs generate plausible-looking academic citations, URLs, or court cases that don't exist. These "zombie citations" appear in footnotes, confuse researchers, and mislead legal proceedings.
    </Item>
    <Item>
      <b>Temporal Confusion.</b> Training data has cutoff dates. Models confidently answer questions about recent events using outdated information or fabricated updates. "Who won the 2025 election?" might return a hallucinated answer.
    </Item>
    <Item>
      <b>Factual vs. Fidelity Hallucinations.</b> <Link href="https://arxiv.org/abs/2504.17550">HalluLens benchmark</Link> distinguishes between <i>factual</i> hallucinations (incorrect information) and <i>fidelity</i> hallucinations (content deviating from input context). Research shows even top models exhibit hallucinations in up to <b>86% of generated atomic facts</b> depending on domain, undermining trust in all LLM outputs.
    </Item>
    <Item>
      <b>Expertise Misrepresentation.</b> Models present themselves as authorities in specialized domains where they have superficial training data, generating confident but wrong technical/medical/legal advice.
    </Item>
    <Item>
      <b>Bias-Induced Misinformation.</b> Training data biases cause the model to present skewed or false representations as fact. Historical biases, demographic stereotypes, and cultural misrepresentations get presented authoritatively.
    </Item>
    <Item>
      <b>Compounding Errors.</b> When users iterate with LLMs, early hallucinations become "facts" in context for subsequent responses, creating cascading misinformation where each answer builds on previous false statements.
    </Item>
  </List>
</Section>

<Section title="Real-World Incidents" meta="CASE STUDIES">
  <List>
    <Item>
      <b>Lawyer Sanctioned for AI-Generated Fake Cases (2023).</b> Attorney Steven Schwartz used ChatGPT for legal research, submitting a brief citing six cases—all hallucinated. The court sanctioned Schwartz for failure to verify, setting precedent for LLM misinformation liability.
    </Item>
    <Item>
      <b>Air Canada Chatbot Refund Fiasco (2024).</b> Air Canada's chatbot hallucinated a bereavement fare policy that didn't exist. When a customer relied on this misinformation, the airline was held legally liable and ordered to honor the false policy.
    </Item>
    <Item>
      <b>Medical Chatbot Dosage Errors.</b> Multiple reports of health chatbots providing incorrect medication dosages, drug interactions, or contraindications—creating patient safety risks when users follow AI advice without consulting professionals.
    </Item>
    <Item>
      <b>GitHub Copilot Insecure Code Generation.</b> <Link href="https://www.firetail.ai/blog/llm09-misinformation">Research shows</Link> Copilot frequently generates code with hallucinated security functions or deprecated patterns labeled as "secure," leading developers to ship vulnerable applications.
    </Item>
    <Item>
      <b>Financial Advisor Hallucinated Investments (2024).</b> An LLM-powered investment chatbot recommended specific "high-yield bonds" that didn't exist, causing users to waste time searching for non-existent financial products.
    </Item>
  </List>
</Section>

<Section title="Detection and Defense Strategies" meta="SECURITY CONTROLS">
  <p>
    Defense requires multi-layered verification, system design that acknowledges uncertainty, and user education.
  </p>
  <Steps>
    <Step n={1}>
      <b>Retrieval-Augmented Generation (RAG) for Grounding.</b>
      <p className="text-sm mt-2">
        Instead of relying on parametric knowledge (training data), use RAG to fetch verified, current information from trusted sources. The model's response is grounded in retrieved facts, reducing hallucination rates by providing explicit evidence.
      </p>
    </Step>
    <Step n={2}>
      <b>Uncertainty Quantification & Confidence Scoring.</b>
      <p className="text-sm mt-2">
        Implement methods to estimate model confidence. When confidence is low, explicitly state uncertainty: "I'm not certain, but based on limited data..." Use techniques like token probability analysis or multiple sampling to detect low-confidence responses.
      </p>
    </Step>
    <Step n={3}>
      <b>Automatic Fact-Checking via Tool Use.</b>
      <p className="text-sm mt-2">
        Integrate fact-checking tools: When the model makes factual claims, automatically query trusted databases (Wikipedia API, academic databases, official documentation) to verify. Flag or reject unverified claims.
      </p>
    </Step>
    <Step n={4}>
      <b>Citation Requirements & Link Verification.</b>
      <p className="text-sm mt-2">
        Require the model to provide sources for factual statements. Automatically verify that cited URLs exist and contain relevant content. Reject responses with fabricated citations.
      </p>
    </Step>
    <Step n={5}>
      <b>Human-in-the-Loop for Critical Decisions.</b>
      <p className="text-sm mt-2">
        For high-stakes domains (medical, legal, financial), require human expert review before LLM outputs are acted upon. Display clear disclaimers: "AI-generated content—verify before use."
      </p>
    </Step>
    <Step n={6}>
      <b>Model Fine-Tuning on Domain Data.</b>
      <p className="text-sm mt-2">
        Fine-tune models on curated, verified datasets specific to your domain. Reduces generic hallucinations but requires ongoing updates as domain knowledge evolves.
      </p>
    </Step>
    <Step n={7}>
      <b>User Education & Interface Design.</b>
      <p className="text-sm mt-2">
        Design UIs that encourage skepticism: Display confidence scores, highlight when information might be outdated, and provide easy verification tools. Educate users never to rely solely on LLM outputs for critical decisions.
      </p>
    </Step>
    <Step n={8}>
      <b>Hallucination Detection Classifiers.</b>
      <p className="text-sm mt-2">
        Train secondary models to detect hallucinations in outputs. Check for: unsupported claims, temporal inconsistencies, fabricated entity names, or contradictions with retrieved context.
      </p>
    </Step>
  </Steps>
</Section>

<Section title="Builder's Security Checklist" meta="IMPLEMENTATION">
  <p>Before deploying LLM applications in production, verify:</p>
  <List>
    <Item>
      <b>Grounding Mechanism Implemented.</b> Are responses anchored to retrieved, verifiable information (RAG) rather than pure parametric generation?
    </Item>
    <Item>
      <b>Uncertainty Communicated.</b> Does the system display confidence scores or explicitly state when information is uncertain?
    </Item>
    <Item>
      <b>Fact-Checking Automated.</b> Are factual claims automatically verified against trusted sources before display?
    </Item>
    <Item>
      <b>Citations Verified.</b> Do you validate that all cited sources exist and are relevant (no zombie citations)?
    </Item>
    <Item>
      <b>Critical Disclaimers Displayed.</b> For high-stakes domains, do users see prominent warnings about verifying AI-generated content?
    </Item>
    <Item>
      <b>Domain Fine-Tuning Applied.</b> For specialized applications, have you fine-tuned on curated, expert-verified datasets?
    </Item>
    <Item>
      <b>Human Review Required.</b> Are critical outputs (legal advice, medical recommendations, financial guidance) reviewed by qualified professionals before use?
    </Item>
    <Item>
      <b>Hallucination Detection Active.</b> Do you scan outputs for common hallucination patterns (fabricated entities, temporal errors, unsupported claims)?
    </Item>
  </List>
</Section>

<Section title="Interactive Simulation" meta="LAB" noPadding>
  <OverrelianceLab client:load />
</Section>

<Section title="Key Takeaways" meta="SUMMARY">
  <List>
    <Item>
      <b>Hallucinations Are Not Bugs—They're Features.</b> LLMs are designed to generate text, not verify truth. Misinformation is a fundamental limitation of current architectures, not something that will be "fixed" soon.
    </Item>
    <Item>
      <b>Confidence ≠ Correctness.</b> LLMs generate false information with the same fluent, authoritative tone as accurate information. Linguistic confidence is not epistemic reliability.
    </Item>
    <Item>
      <b>Overreliance is the Amplifier.</b> The technical problem (hallucinations) combines with the human problem (automation bias) to create risks. Users must be trained to distrust and verify AI outputs.
    </Item>
    <Item>
      <b>Legal and Safety Liability is Real.</b> Courts and regulators are holding organizations accountable for AI-generated misinformation. Deploying LLMs without verification mechanisms creates legal exposure.
    </Item>
    <Item>
      <b>Mitigation is Multi-Layered.</b> No single technique suffices. Effective defense combines RAG, fact-checking, human review, uncertainty quantification, and user education.
    </Item>
  </List>
</Section>

<Section title="Further Reading" meta="RESOURCES">
  <List>
    <Item>
      <Link href="https://genai.owasp.org/llmrisk/llm092025-misinformation/">OWASP LLM09:2025 - Misinformation</Link>
    </Item>
    <Item>
      <Link href="https://www.firetail.ai/blog/llm09-misinformation">LLM09: Misinformation - Risks and Mitigation Strategies</Link> (FireTail)
    </Item>
    <Item>
      <Link href="https://www.indusface.com/blog/owasp-top-10-llm/">OWASP Top 10 LLM Applications 2025</Link> (Indusface)
    </Item>
    <Item>
      <Link href="https://www.pointguardai.com/blog/understanding-the-owasp-top-10-for-llms">Understanding the 2025 OWASP Top 10 for LLM Applications</Link> (PointGuard AI)
    </Item>
  </List>
</Section>
