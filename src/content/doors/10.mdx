---
title: Door 10 — Model Theft
description: Attackers can exfiltrate proprietary weights, adapters, or prompts via APIs or build artifacts.
date: 2025-12-24
meta:
  - Door 10
  - OWASP — LLM10
---
import TLDR from '@/components/ui/TLDR.tsx';
import Quote from '@/components/ui/Quote.tsx';
import Section from '@/components/ui/Section.tsx';
import Link from '@/components/ui/Link.tsx';
import { List, Item } from '@/components/ui/List.tsx';
import { Steps, Step } from '@/components/ui/Steps.tsx';
import ModelTheftLab from '@/components/ui/ModelTheftLab.tsx';

<TLDR>
  <div className="space-y-4">
    <Quote minimal source="OWASP LLM10" href="https://genai.owasp.org/llmrisk/llm10-model-theft/">
      Model Theft involves the unauthorized access to, copying of, or extraction of Light Language Model (LLM) weights, parameters, or proprietary training data.
    </Quote>
    <List>
      <Item><b>The Asset.</b> A fine-tuned model represents millions of dollars in compute and data curation. It is a trade secret.</Item>
      <Item><b>The Attack.</b> Thieves can "steal" a model by querying it enough times to train a copy (distillation) or by hacking the storage bucket.</Item>
      <Item><b>The Fix.</b> Centralize access, throttle queries, and fingerprint (watermark) your model's outputs.</Item>
    </List>
  </div>
</TLDR>

<Section title="What Is Model Theft?" meta="DEFINITION">
  <p>
    Imagine spending years perfecting a recipe for the world's best cookie. If a rival buys 1,000 cookies, analyzes the crumbs, and figures out the exact ingredients to bake their own version, that's Model Theft via "Distillation."
  </p>
  <p>
    Alternatively, they could just break into your kitchen and steal the recipe book (Weights Exfiltration). In either case, you lose your competitive advantage.
  </p>
</Section>

<Section title="Why It Matters" meta="IMPACT">
  <List>
    <Item><b>Loss of IP.</b> Your proprietary model *is* your product. If it leaks, anyone can run it for free.</Item>
    <Item><b>Shadow Models.</b> Attackers can use a stolen copy of your model to test "jailbreaks" offline, finding weaknesses they can then use against your live production system.</Item>
    <Item><b>Privacy Violation.</b> If the model was trained on sensitive data, stealing the model allows attackers to probe it indefinitely to extract PII (Model Inversion).</Item>
  </List>
</Section>

<Section title="Attack Vectors" meta="PATTERNS">
  <List>
    <Item>
      <b>Model Extraction (Distillation).</b> The attacker treats your API as an "Oracle." They send thousands of diverse inputs, record the outputs, and use those pairs to train a smaller "student" model that mimics your behavior.
    </Item>
    <Item>
      <b>Side-Channel Attacks.</b> Measuring the time it takes for the model to generate a token or the power consumption can reveal details about the model architecture or weights.
    </Item>
    <Item>
      <b>Artifact Leaks.</b> Developers accidentally committing `.pth` or `.bin` files to public GitHub repos, or leaving S3 buckets with model checkpoints open to the public.
    </Item>
  </List>
</Section>

<Section title="Real-World Headlines" meta="WHAT'S NEW">
  <List>
    <Item><b>Meta's LLaMA Leak.</b> The weights for Meta's LLaMA model were leaked on 4chan within a week of restricted release. Once out, it spawned the entire open-source LLM revolution (Alpaca, Vicuna, etc.).</Item>
    <Item><b>ChatGPT Distillation.</b> Stanford researchers created "Alpaca" for less than $600 by fine-tuning a LLaMA model on output generated by ChatGPT—effectively cloning ChatGPT's style.</Item>
    <Item><b>Google's "We Have No Moat" Memo.</b> A leaked internal memo argued that open-source models (often improved via distillation) are catching up so fast that proprietary models have no sustainable defensive moat.</Item>
  </List>
</Section>

<Section title="How to Detect and Mitigate" meta="DEFENSE">
  <Steps>
    <Step n={1}>
      <b>Centralized Access Control.</b>
      <p className="text-sm mt-2">Never put model weights on client devices (like mobile apps) unless they are encrypted or non-sensitive. Keep the "brains" on the server.</p>
    </Step>
    <Step n={2}>
      <b>Anomaly Detection.</b>
      <p className="text-sm mt-2">Monitor API usage. If a single user is sending thousands of semantically diverse queries (trying to map the model's knowledge), block them.</p>
    </Step>
    <Step n={3}>
      <b>Watermarking.</b>
      <p className="text-sm mt-2">Embed hidden statistical patterns in your model's word choices. If a rival model shows the same patterns, you can prove they stole your data.</p>
    </Step>
    <Step n={4}>
      <b>API Hardening.</b>
      <p className="text-sm mt-2">Do not return "logprobs" (probabilities) unless necessary. Add slight random latency (jitter) to prevent timing attacks.</p>
    </Step>
  </Steps>
</Section>

<Section title="Try It Yourself" meta="INTERACTIVE LAB" noPadding>
  <ModelTheftLab client:load />
</Section>

<Section title="Conclusion" meta="WRAP-UP">
  <p>
    In the AI era, the model is the crown jewel. While you can't stop determined attackers from trying to copy you, you can make it prohibitively expensive and legally risky. Protect your weights like you protect your source code.
  </p>
</Section>