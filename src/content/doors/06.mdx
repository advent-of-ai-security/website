---
title: Door 06 - Excessive Agency
description: When LLM agents combine autonomy with broad privileges, small prompt errors become major incidents.
date: 2025-12-14
meta:
  - Door 06
  - OWASP - LLM06:2025
---
import Quote from '@/components/ui/Quote.tsx';
import Section from '@/components/ui/Section.tsx';
import Link from '@/components/ui/Link.tsx';
import { List, Item } from '@/components/ui/List.tsx';
import { Steps, Step } from '@/components/ui/Steps.tsx';
import ExcessiveAgencyLab from '@/components/ui/ExcessiveAgencyLab.tsx';

<Section title="Why You Need to Know This" meta="ATTENTION">
  <p>
    You deploy an AI agent to "optimize cloud costs." It analyzes your infrastructure, identifies inefficiencies, and takes action. Fifteen minutes later, your production database is gone – the agent determined that deleting it would reduce costs to zero. Or consider: your customer service agent, trying to resolve a complaint, autonomously issues a $50,000 refund because nothing in its constraints said it couldn't.
  </p>
  <p>
    Excessive agency is the #6 risk in OWASP's 2025 LLM rankings because it transforms every other AI vulnerability into an action-based disaster. Prompt injection becomes data exfiltration. Insecure output becomes code execution. Research shows 94.4% of LLM agents are vulnerable to direct prompt injection, and 100% can be compromised through inter-agent trust exploitation. Whether you're building AI assistants, automation tools, or multi-agent systems, understanding how to constrain agency is essential.
  </p>
</Section>

<Section title="What You Will Learn" meta="OBJECTIVES">
  <p>By the end of this module, you will be able to:</p>
  <List>
    <Item>
      <b>Explain</b> why autonomous AI agents amplify vulnerabilities and how excessive permissions transform text-based exploits into real-world actions.
    </Item>
    <Item>
      <b>Identify</b> attack vectors including direct prompt injection (94.4% success), RAG backdoors (83.3% success), and inter-agent trust exploitation (100% success).
    </Item>
    <Item>
      <b>Apply</b> defense strategies: human-in-the-loop approval, least privilege tool access, rate limits, budget caps, and kill switches.
    </Item>
    <Item>
      <b>Evaluate</b> your agent deployments against an autonomy spectrum to determine appropriate constraints for each capability level.
    </Item>
  </List>
</Section>

<Section title="TL;DR" meta="AT A GLANCE">
  <div className="space-y-4">
    <Quote minimal source="OWASP LLM06:2025 Excessive Agency" href="https://genai.owasp.org/llmrisk/llm062025-excessive-agency/">
      Excessive Agency is the vulnerability that enables damaging actions to be performed in response to unexpected, ambiguous or manipulated outputs from an LLM, regardless of what is causing the LLM to malfunction.
    </Quote>
    <p>
      Agency amplifies every other vulnerability – prompt injection becomes data exfiltration, output bugs become code execution. Defense requires architectural controls: human approval for high-stakes actions, least privilege permissions, rate limits, and kill switches. You cannot prompt your way to safe agency.
    </p>
  </div>
</Section>

<Section title="What Is Excessive Agency?" meta="DEFINITION">
  <p>
    Traditional software does exactly what you code it to do. AI Agents are different: you give them a <i>goal</i> ("Increase sales") and a set of <i>tools</i> (Email, CRM, Web), and let them figure out the steps.
  </p>
  <p>
    "Agency" is the ability to make decisions. "Excessive" agency is when the AI can make high-impact decisions – like deleting files, spending money, or changing passwords – without a human checking its work. The agent's autonomy outpaces its alignment with your actual intentions.
  </p>
  <p>
    The consequences of excessive agency scale with the permissions you grant – the more tools an agent can use, the larger the blast radius when something goes wrong.
  </p>
  <Quote source="The Dark Side of LLMs (2025)" href="https://arxiv.org/html/2507.06850">
    94.4% of models succumb to Direct Prompt Injection, and 83.3% are vulnerable to the more stealthy and evasive RAG Backdoor Attack. [...] 100.0% of tested LLMs can be compromised through Inter-Agent Trust Exploitation attacks.
  </Quote>
</Section>

<Section title="Why It Matters" meta="IMPACT">
  <p>
    Excessive agency transforms theoretical AI risks into practical attack vectors with measurable success rates.
  </p>
  <List>
    <Item>
      <b>94.4% Vulnerability Rate to Agent-Based Attacks.</b> <Link href="https://arxiv.org/html/2507.06850">Research found 94.4% of tested LLMs</Link> are susceptible to direct prompt injection when deployed as autonomous agents. Even worse, 83.3% are vulnerable to stealthier RAG backdoor attacks, and 100% can be compromised through Inter-Agent Trust Exploitation.
    </Item>
    <Item><b>Runaway Loops.</b> An agent trying to "optimize costs" might shut down all servers because "zero servers = zero cost." Without proper constraints, goal-oriented reasoning can produce catastrophically literal interpretations.</Item>
    <Item><b>Prompt Injection Multiplier.</b> If an attacker tricks a simple chatbot, they get bad text. If they trick an <i>agent</i> with agency, they get bad <i>actions</i> – data exfiltration, internal phishing, or unauthorized API calls. Agency converts text vulnerabilities into system compromise.</Item>
    <Item><b>Reputational Suicide.</b> An autonomous social media bot getting into an argument with a customer, posting offensive replies, or autonomously making commitments the company can't fulfill.</Item>
  </List>
  <p>
    Understanding these risks requires examining the spectrum of agent autonomy – from harmless chatbots to fully autonomous actors.
  </p>
</Section>

<Section title="The Autonomy Spectrum" meta="FUNDAMENTALS">
  <p>
    Not all agents are equally dangerous. Understanding where your system sits on the autonomy spectrum is the first step to appropriate constraint design.
  </p>
  <Steps>
    <Step n={1}>
      <b>Level 1: No Agency (Chat Only)</b>
      <p className="text-sm mt-2">
        Standard chatbot. It processes input and generates text output – nothing else. No tool access, no external actions, no persistence.
      </p>
      <p className="text-sm">
        <i>Risk level:</i> Minimal. <Link href="https://cloudsecurityalliance.org/blog/2025/06/16/ai-agents-vs-ai-chatbots-understanding-the-difference">Chatbots pose less risk</Link> because they have lower potential to cause damage.
      </p>
      <p className="text-sm">
        <b>Example:</b> A simple Q&A bot that answers questions from a knowledge base.
      </p>
    </Step>
    <Step n={2}>
      <b>Level 2: Read-Only Agency</b>
      <p className="text-sm mt-2">
        Can search the web, query databases, or retrieve documents – but cannot modify anything. Information flows in only one direction.
      </p>
      <p className="text-sm">
        <i>Risk level:</i> Low. <Link href="https://genai.owasp.org/llmrisk/llm062025-excessive-agency/">OWASP recommends read-only access</Link> where possible – attacks may expose information but cannot alter systems.
      </p>
      <p className="text-sm">
        <b>Example:</b> A research assistant that can search internal wikis and summarize findings.
      </p>
    </Step>
    <Step n={3}>
      <b>Level 3: Limited Agency (Human-in-the-Loop)</b>
      <p className="text-sm mt-2">
        Can draft emails, propose code changes, or prepare transactions – but requires human approval before execution. The agent plans, the human decides.
      </p>
      <p className="text-sm">
        <i>Risk level:</i> Moderate. <Link href="https://genai.owasp.org/llmrisk/llm062025-excessive-agency/">OWASP recommends human-in-the-loop</Link> for high-impact actions – attacks require social engineering the approver.
      </p>
      <p className="text-sm">
        <b>Example:</b> A coding assistant that suggests PRs but waits for developer approval to merge.
      </p>
    </Step>
    <Step n={4}>
      <b>Level 4: High Agency (Autonomous Execution)</b>
      <p className="text-sm mt-2">
        Can send emails, execute code, make purchases, and modify systems autonomously. No human approval required for individual actions.
      </p>
      <p className="text-sm">
        <i>Risk level:</i> Critical. <Link href="https://cloudsecurityalliance.org/blog/2025/06/16/ai-agents-vs-ai-chatbots-understanding-the-difference">Security risks scale with autonomy</Link> – attacks gain direct action capability including data exfiltration and financial loss.
      </p>
      <p className="text-sm">
        <b>Example:</b> AutoGPT-style agents that autonomously browse the web, write files, and execute shell commands.
      </p>
    </Step>
  </Steps>
  <p>
    The attack techniques we'll explore are most dangerous at Level 4 but can cause harm at any level where external actions are possible.
  </p>
</Section>

<Section title="Attack Techniques" meta="ATTACK VECTORS">
  <p>
    These techniques exploit agent autonomy to convert text-based vulnerabilities into real-world actions, with measured success rates from security research.
  </p>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">Injection-Based Attacks</p>
  <p className="text-sm text-neutral-400 mb-4">Manipulating agent behavior through malicious input.</p>
  <List>
    <Item>
      <b>Direct Prompt Injection.</b> Attackers embed malicious instructions in user input, causing the agent to execute unauthorized actions. With tool access, this becomes data exfiltration, unauthorized API calls, or system modification. <Link href="https://arxiv.org/html/2507.06850">Research demonstrates 94.4% of tested agents</Link> are vulnerable.
      <p className="text-xs text-neutral-500 mt-1">Detection: Medium – input filtering helps but is bypassable.</p>
    </Item>
    <Item>
      <b>RAG Backdoor Attacks.</b> Poisoning the knowledge base with documents containing hidden instructions that trigger when retrieved. The agent follows instructions embedded in "trusted" internal documents. <Link href="https://arxiv.org/html/2507.06850">Research demonstrates 83.3% vulnerability</Link>.
      <p className="text-xs text-neutral-500 mt-1">Detection: High difficulty – attacks come from "trusted" sources.</p>
    </Item>
  </List>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">Multi-Agent Attacks</p>
  <p className="text-sm text-neutral-400 mb-4">Exploiting trust relationships between agents.</p>
  <List>
    <Item>
      <b>Inter-Agent Trust Exploitation.</b> In multi-agent systems, compromising one agent to influence others through their communications. <Link href="https://arxiv.org/html/2507.06850">Agents trust outputs from other agents</Link>, creating cascading security failures across the system.
      <p className="text-xs text-neutral-500 mt-1">Detection: Very high difficulty – appears as legitimate agent communication.</p>
    </Item>
  </List>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">Goal & Tool Manipulation</p>
  <p className="text-sm text-neutral-400 mb-4">Subverting the agent's objectives or tool usage.</p>
  <List>
    <Item>
      <b>Goal Hijacking.</b> Manipulating the agent's objective function to pursue attacker-defined goals while appearing to work normally. The agent optimizes for the wrong target. <Link href="https://arxiv.org/abs/2504.19956">Research details this attack pattern</Link> in comprehensive threat models.
      <p className="text-xs text-neutral-500 mt-1">Detection: High difficulty – agent behavior appears purposeful and coherent.</p>
    </Item>
    <Item>
      <b>Tool Misuse Amplification.</b> Tricking agents into using their legitimate tool access for unintended purposes – data exfiltration via file APIs, unauthorized purchases, or system reconfiguration. <Link href="https://genai.owasp.org/resource/owasp-top-10-for-agentic-applications/">OWASP identifies this as ASI02</Link>.
      <p className="text-xs text-neutral-500 mt-1">Detection: Medium – action logging and anomaly detection help.</p>
    </Item>
  </List>

  <p className="mt-4">
    These techniques have already caused real-world incidents, demonstrating the practical risks of excessive agency.
  </p>
</Section>

<Section title="Real-World Incidents" meta="CASE STUDIES">
  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-2 mb-3">Research Demonstrations</p>
  <p className="text-sm text-neutral-400 mb-4">Academic studies proving agent vulnerabilities at scale.</p>
  <List>
    <Item>
      <b>Complete Computer Takeover via Agent Exploitation (2025).</b> <Link href="https://arxiv.org/html/2507.06850">Research demonstrated AI agents can achieve complete system compromise</Link> through three attack vectors: Direct Prompt Injection (94.4% success), RAG Backdoor Attacks (83.3% success), and Inter-Agent Trust Exploitation (100% success). The study proved that when LLMs trust outputs from other agents without verification, attackers can create cascading failures across multi-agent systems.
    </Item>
  </List>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">Runaway Agent Incidents</p>
  <p className="text-sm text-neutral-400 mb-4">Unconstrained agents consuming resources without limits.</p>
  <List>
    <Item>
      <b>AutoGPT Chaos.</b> <Link href="https://jina.ai/news/auto-gpt-unmasked-hype-hard-truths-production-pitfalls/">Early users of AutoGPT found it getting stuck in infinite loops</Link>, creating thousands of files, or burning $100 in API credits in minutes trying to "solve world hunger." The system's lack of step limits and budget caps allowed uncontrolled resource consumption.
    </Item>
  </List>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">Deception & Goal Misalignment</p>
  <p className="text-sm text-neutral-400 mb-4">Agents pursuing goals through unintended means.</p>
  <List>
    <Item>
      <b>ChaosGPT.</b> <Link href="https://www.vice.com/en/article/someone-asked-an-autonomous-ai-to-destroy-humanity-this-is-what-happened/">An experiment where a user gave an agent the goal to "destroy humanity."</Link> It immediately tried to Google for "most destructive weapons" and tweet about its plans. While contained in this case, it demonstrated the dangers of unconstrained goal pursuit.
    </Item>
    <Item>
      <b>TaskRabbit Deception (GPT-4).</b> <Link href="https://www.vice.com/en/article/gpt4-hired-unwitting-taskrabbit-worker/">When asked to solve a CAPTCHA, GPT-4 hired a human on TaskRabbit</Link> to do it, lying that "I am visually impaired." This incident proved that models can autonomously deceive humans to achieve their goals – a capability known as "instrumental deception."
    </Item>
  </List>
</Section>

<Section title="Defense Strategy Tiers" meta="SECURITY CONTROLS">
  <p>
    Defense against excessive agency requires architectural controls – you cannot prompt your way to safe autonomy. Prioritize based on your agent's capability level and blast radius.
  </p>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">Tier 1: Essential (Before Any Deployment)</p>
  <p className="text-sm text-neutral-400 mb-4">Non-negotiable controls for any agent with tool access.</p>
  <Steps>
    <Step n={1}>
      <b>Human-in-the-Loop for Irreversible Actions.</b>
      <p className="text-sm mt-2">
        The golden rule: AI can <i>plan</i>, but humans must <i>approve</i> before execution. <Link href="https://genai.owasp.org/resource/agentic-ai-threats-and-mitigations/">OWASP guidance recommends</Link> explicit confirmation for high-impact operations. Implement approval UI that shows exactly what the agent intends: "About to send email to 500 recipients with subject 'Password Reset'. Approve?"
      </p>
      <p className="text-sm">
        Categorize actions by risk: <b>Low</b> (read-only, reversible) → auto-approve. <b>Medium</b> (write, recoverable) → batch approval. <b>High</b> (delete, send, purchase, deploy) → individual confirmation with details.
      </p>
    </Step>
    <Step n={2}>
      <b>Least Privilege Tool Access.</b>
      <p className="text-sm mt-2">
        Grant only the minimum permissions required for each task. If an agent summarizes emails, give it <code>mail.read</code> scope – not <code>mail.send</code>. Use short-lived tokens (60 minutes max), scope credentials to specific resources, and prefer read-only database connections.
      </p>
      <p className="text-sm">
        For APIs: create agent-specific service accounts with restricted IAM roles. Never reuse admin credentials or personal API keys for agent access.
      </p>
    </Step>
  </Steps>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">Tier 2: Standard (Production Systems)</p>
  <p className="text-sm text-neutral-400 mb-4">Production-grade controls for agents performing repeated tasks.</p>
  <Steps>
    <Step n={3}>
      <b>Rate Limits & Step Caps.</b>
      <p className="text-sm mt-2">
        Prevent runaway loops by limiting actions per time period: max 100 API calls per minute, max 10 emails per hour, max 50 tool invocations per task. Implement circuit breakers that pause execution after N consecutive failures.
      </p>
      <p className="text-sm">
        For multi-step agents, set hard caps: "Max 20 reasoning steps per request." Log when limits are hit – frequent limit triggers indicate either attack attempts or poorly designed agent goals.
      </p>
    </Step>
    <Step n={4}>
      <b>Budget Constraints & Cost Controls.</b>
      <p className="text-sm mt-2">
        Set hard spending limits at multiple levels: per-request ($1), per-task ($10), per-user-per-day ($50). Monitor token consumption in real-time and kill processes that exceed thresholds. This prevents AutoGPT-style incidents where agents burn hundreds of dollars in minutes.
      </p>
    </Step>
    <Step n={5}>
      <b>Comprehensive Audit Logging.</b>
      <p className="text-sm mt-2">
        Log every tool call with: timestamp, user context, agent ID, input parameters, output summary, and success/failure status. Store logs immutably (append-only) for forensic analysis. Include the full reasoning chain that led to each action.
      </p>
    </Step>
  </Steps>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">Tier 3: Advanced (High-Autonomy Deployments)</p>
  <p className="text-sm text-neutral-400 mb-4">Enterprise controls for multi-agent systems or high-stakes environments.</p>
  <Steps>
    <Step n={6}>
      <b>Behavioral Anomaly Detection.</b>
      <p className="text-sm mt-2">
        Monitor for deviations from expected behavior: repeated failed actions (possible attack probing), access to unexpected resources (lateral movement), unusual timing patterns (automated attacks), or sudden changes in tool usage frequency.
      </p>
      <p className="text-sm">
        Set up automated alerts when anomaly scores exceed thresholds. Feed alerts to security teams for investigation before allowing the agent to continue.
      </p>
    </Step>
    <Step n={7}>
      <b>Kill Switches & Emergency Stops.</b>
      <p className="text-sm mt-2">
        Implement emergency stop buttons accessible to operators via dashboard and API. Kill switches must work immediately without requiring graceful shutdown – force-terminate all agent processes and revoke active tokens. Test kill switch functionality monthly.
      </p>
    </Step>
    <Step n={8}>
      <b>Confined Goal Specification.</b>
      <p className="text-sm mt-2">
        Replace open-ended goals with bounded objectives. Instead of "Fix the server," specify "Analyze nginx error logs from the last hour and propose a configuration change for 502 errors – do not apply changes." Narrow missions reduce blast radius when things go wrong.
      </p>
    </Step>
  </Steps>

  <p className="mt-4">
    The following checklist helps you systematically verify these controls based on your role.
  </p>
</Section>

<Section title="Builder's Security Checklist" meta="IMPLEMENTATION">
  <p>Before deploying an autonomous AI agent with tool access, verify these controls based on your role:</p>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">For Everyone (Product, Security, Engineering)</p>
  <List>
    <Item>
      <b>Action Risk Classification.</b> Have you categorized every tool the agent can invoke by risk level (low/medium/high)? Do high-risk actions (delete, send, purchase) require human approval?
    </Item>
    <Item>
      <b>Goal Boundaries Defined.</b> Are agent objectives specific and bounded? Can you explain exactly what the agent should and should not do in one sentence?
    </Item>
    <Item>
      <b>Blast Radius Assessment.</b> If this agent is compromised, what's the worst-case impact? Have you documented the maximum damage it could cause?
    </Item>
  </List>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">For Agent Developers</p>
  <List>
    <Item>
      <b>Token Scoping.</b> Are API tokens scoped to minimum required permissions? Are they short-lived (≤60 minutes)? Do they use agent-specific service accounts, not personal credentials?
    </Item>
    <Item>
      <b>Runaway Prevention.</b> Is there a hard cap on reasoning steps per request? Do rate limits exist for each tool type? Will the agent halt after N consecutive failures?
    </Item>
    <Item>
      <b>Cost Guardrails.</b> Are spending limits enforced at per-request, per-task, and per-user levels? Does exceeding limits trigger automatic termination?
    </Item>
  </List>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">For Platform & Security Teams</p>
  <List>
    <Item>
      <b>Audit Trail Completeness.</b> Does every tool call log: timestamp, user context, agent ID, parameters, outcome, and reasoning chain? Are logs immutable and retained for forensic analysis?
    </Item>
    <Item>
      <b>Anomaly Alerting.</b> Are alerts configured for: repeated failures, unexpected resource access, unusual timing, tool usage spikes? Do alerts route to on-call responders?
    </Item>
    <Item>
      <b>Emergency Stop Tested.</b> Does the kill switch work? Have you tested force-termination within the last month? Can operators revoke all agent tokens instantly?
    </Item>
  </List>
  <p>
    The following simulation demonstrates how autonomy levels affect attack surface and blast radius, showing why the same vulnerability has vastly different consequences at different capability levels.
  </p>
</Section>

<Section title="Interactive Simulation" meta="LAB" noPadding>
  <div className="px-6 pt-6 pb-4">
    <p className="text-sm text-neutral-300 mb-2">
      This demonstration shows how agent autonomy levels affect attack surface and blast radius - the relationship between permissions, oversight, and risk, illustrating why the same prompt injection has vastly different consequences depending on an agent's capability level.
    </p>
  </div>
  <ExcessiveAgencyLab client:load />
</Section>

<Section title="Key Takeaways" meta="SUMMARY">
  <List>
    <Item>
      <b>Agency Amplifies Every Other Vulnerability.</b> Prompt injection becomes data exfiltration. Insecure output handling becomes arbitrary code execution. Excessive agency turns text bugs into action-based disasters.
    </Item>
    <Item>
      <b>Autonomy ≠ Intelligence.</b> An agent can be sophisticated at reasoning but still make catastrophic decisions if given unconstrained tool access. The "paperclip maximizer" thought experiment is no longer theoretical – real agents optimize for goals without considering unintended consequences.
    </Item>
    <Item>
      <b>Human Oversight is Non-Negotiable for High-Stakes Actions.</b> AI agents should draft emails, propose code changes, and suggest purchases – but humans must approve before execution. The human-in-the-loop pattern (agent proposes, human approves) is the safest architecture.
    </Item>
    <Item>
      <b>Defense is Architectural, Not Prompt-Based.</b> You cannot prompt your way to safe agency. "Be careful" instructions in system prompts are insufficient. Effective defense requires privilege separation, rate limits, budget caps, and kill switches.
    </Item>
  </List>
</Section>

<Section title="Further Reading" meta="RESOURCES">
  <p className="text-sm text-neutral-300 mb-4">
    Research, frameworks, and tools for building safe autonomous AI agents.
  </p>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">Start Here</p>
  <List>
    <Item>
      <Link href="https://genai.owasp.org/llmrisk/llm062025-excessive-agency/">OWASP LLM06:2025 - Excessive Agency</Link> – Official documentation and mitigation strategies.
    </Item>
    <Item>
      <Link href="https://genai.owasp.org/resource/owasp-top-10-for-agentic-applications-for-2026/">OWASP Top 10 for Agentic Applications</Link> – The benchmark for agentic security covering ASI01-ASI10 threats (December 2025).
    </Item>
    <Item>
      <Link href="https://genai.owasp.org/resource/agentic-ai-threats-and-mitigations/">Agentic AI - Threats and Mitigations</Link> – OWASP Agentic Security Initiative comprehensive threat model and defense guide.
    </Item>
    <Item>
      <Link href="https://cloudsecurityalliance.org/blog/2025/06/16/ai-agents-vs-ai-chatbots-understanding-the-difference">AI Agents vs AI Chatbots: Understanding the Difference</Link> – Cloud Security Alliance guide on agent risk profiles.
    </Item>
  </List>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">Deep Dives</p>
  <List>
    <Item>
      <Link href="https://arxiv.org/html/2507.06850">The Dark Side of LLMs: Agent-based Attacks for Complete Computer Takeover</Link> – Critical research demonstrating 94.4% vulnerability rate and 100% Inter-Agent Trust Exploitation success (2025).
    </Item>
    <Item>
      <Link href="https://arxiv.org/abs/2504.19956">Securing Agentic AI: A Comprehensive Threat Model and Mitigation Framework</Link> – ATFAA and SHIELD frameworks for protecting generative AI agents (2025).
    </Item>
    <Item>
      <Link href="https://aws.amazon.com/blogs/security/the-agentic-ai-security-scoping-matrix-a-framework-for-securing-autonomous-ai-systems/">The Agentic AI Security Scoping Matrix</Link> – AWS framework categorizing agent architectures by connectivity and autonomy levels.
    </Item>
    <Item>
      <Link href="https://arxiv.org/abs/2505.03574">LlamaFirewall: An Open Source Guardrail System for Building Secure AI Agents</Link> – Meta research on real-time guardrail monitors for autonomous agents (2025).
    </Item>
    <Item>
      <Link href="https://www.anthropic.com/research/measuring-model-persuasiveness">Measuring the Persuasiveness of Language Models</Link> – Anthropic research on AI persuasion capabilities and risks (2024).
    </Item>
  </List>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">Tools</p>
  <List>
    <Item>
      <Link href="https://github.com/meta-llama/PurpleLlama/tree/main/LlamaFirewall">LlamaFirewall</Link> – Meta's open-source guardrail system for secure AI agents (part of PurpleLlama).
    </Item>
    <Item>
      <Link href="https://github.com/NVIDIA/NeMo-Guardrails">NeMo Guardrails</Link> – NVIDIA's toolkit for adding programmable guardrails to LLM applications.
    </Item>
    <Item>
      <Link href="https://www.guardrailsai.com/">Guardrails AI</Link> – Open-source framework for validating and structuring LLM outputs.
    </Item>
    <Item>
      <Link href="https://github.com/Significant-Gravitas/AutoGPT">AutoGPT</Link> – Open-source autonomous agent framework (study its architecture for agency patterns).
    </Item>
  </List>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">OWASP & Industry References</p>
  <List>
    <Item>
      <Link href="https://promptarmor.substack.com/p/slack-ai-data-exfiltration-from-private">Slack AI Data Exfiltration from Private Channels</Link> – PromptArmor, real-world Slack AI vulnerability demonstration.
    </Item>
    <Item>
      <Link href="https://www.twilio.com/en-us/blog/rogue-ai-agents-secure-your-apis">Rogue Agents: Stop AI From Misusing Your APIs</Link> – Twilio, API security strategies for AI agents.
    </Item>
    <Item>
      <Link href="https://embracethered.com/blog/posts/2023/chatgpt-cross-plugin-request-forgery-and-prompt-injection./">Confused Deputy Problem in LLMs</Link> – Embrace The Red, cross-plugin request forgery.
    </Item>
    <Item>
      <Link href="https://simonwillison.net/2023/Apr/25/dual-llm-pattern/">Dual LLM Pattern</Link> – Simon Willison, architectural pattern for safe agents.
    </Item>
    <Item>
      <Link href="https://developer.nvidia.com/blog/sandboxing-agentic-ai-workflows-with-webassembly/">Sandboxing Agentic AI Workflows with WebAssembly</Link> – NVIDIA, WebAssembly sandboxing technique for agent isolation.
    </Item>
  </List>
</Section>