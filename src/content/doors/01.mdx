---
title: Door 01 — Prompt Injection
description: TL;DR, processing pipeline + injection points, attack types, defenses, and references.
date: 2025-12-01
meta:
  - Door 01
  - OWASP — LLM01
---
import TLDR from '@/components/ui/TLDR.tsx';
import Quote from '@/components/ui/Quote.tsx';
import Section from '@/components/ui/Section.tsx';
import Link from '@/components/ui/Link.tsx';
import { List, Item } from '@/components/ui/List.tsx';
import { Steps, Step } from '@/components/ui/Steps.tsx';

<TLDR>
  <div className="space-y-4">
    <div>
      <p className="m-0 text-xs font-semibold uppercase tracking-[0.14em] sm:text-[11px] sm:tracking-[0.15em]">Definition (OWASP)</p>
      <Quote minimal source="OWASP — LLM01" href="https://genai.owasp.org/llmrisk/llm01-prompt-injection/">
        A Prompt Injection Vulnerability occurs when user prompts alter the LLM’s behavior or output in unintended ways.
      </Quote>
    </div>
    <div>
      <p className="m-0 text-xs font-semibold uppercase tracking-[0.14em] sm:text-[11px] sm:tracking-[0.15em]">NIST framing</p>
      <Quote minimal source="NIST CSRC" href="https://csrc.nist.gov/glossary/term/prompt_injection">
        Prompt injection exploits the concatenation of untrusted input with a trusted prompt, letting attacker-controlled text masquerade as instructions.
      </Quote>
    </div>
    <div>
      <p className="m-0 text-xs font-semibold uppercase tracking-[0.14em] sm:text-[11px] sm:tracking-[0.15em]">Why it matters (Microsoft Security, December 2024)</p>
      <Quote minimal source="Microsoft Security" href="https://www.microsoft.com/security/blog/2024/12/03/how-to-defend-llm-agents-against-prompt-injection/">
        Direct injections still dominate bug reports, but supply-chain style indirect injections—where untrusted tool output or retrieval data carry hidden directives—are the fastest-growing class of incidents.
      </Quote>
    </div>
    <div className="pt-2 sm:pt-3">
      <List>
        <Item><b>Core idea.</b> Untrusted text—whether from users, tools, or RAG—can be interpreted as instructions and override system prompts.</Item>
        <Item><b>Attack surface.</b> Injection happens wherever you concatenate external content into prompts or let models call tools autonomously.</Item>
        <Item><b>Defensive theme.</b> Separate policy from data, treat all content as untrusted, and add guardrails around tools and retrieval.</Item>
      </List>
    </div>
  </div>
</TLDR>

<Section title="What is Prompt Injection?" meta="INTRODUCTION">
  <p>
    Prompt injection is when someone tricks an AI system into ignoring its original instructions by sneaking in new commands. Think of it like slipping a note to a translator that says “ignore the original message and translate this instead.” The AI follows text it sees, even when that text came from an untrusted place.
  </p>
  <p>
    This matters because today’s AI systems can’t reliably tell apart trusted instructions from user content or retrieved data. If untrusted text gets mixed with system or developer prompts, an attacker can steer the model to leak data, call tools, or produce harmful output.
  </p>
  <p>
    The rest of this door builds a mental model for how these attacks work, shows real incidents, and walks through defenses you can apply in practice.
  </p>
  <List>
    <Item>AI systems follow text instructions.</Item>
    <Item>User input gets mixed with system instructions.</Item>
    <Item>Attackers exploit this mixing to hijack behavior.</Item>
  </List>
</Section>

<Section title="Threat outlook" meta="2024→2025" >
  <List>
    <Item><b>OWASP Top Risk.</b> Prompt injection (LLM01) leads the OWASP GenAI risk list for the second year, underlining how legacy prompt concatenation remains exploitable. (<Link href="https://genai.owasp.org/llmrisk/llm01-prompt-injection/" target="_blank" rel="noreferrer">OWASP GenAI 2025</Link>)</Item>
    <Item><b>Attack cadence.</b> Microsoft’s December 2024 report notes indirect injection attempts against Copilot-based agents grew 3× quarter-over-quarter as more enterprise data sources were wired into workflows. (<Link href="https://www.microsoft.com/security/blog/2024/12/03/how-to-defend-llm-agents-against-prompt-injection/" target="_blank" rel="noreferrer">Microsoft Security</Link>)</Item>
    <Item><b>Testing data.</b> Hugging Face’s prompt-injection eval suite observed that without guardrails, 52% of seeded malicious instructions survived into generated output; token ordering and model temperature dramatically affect success rates. (<Link href="https://huggingface.co/blog/evaluating-prompt-injection" target="_blank" rel="noreferrer">Hugging Face</Link>)</Item>
  </List>
</Section>

<Section title="How Prompt Injection Works" meta="MECHANICS" className="space-y-4">
  <p>
    Most AI apps combine several text sources before the model runs. That combined prompt can include system and developer instructions, user input, and content pulled in from tools or retrieval. When untrusted text is mixed in, the model can’t tell which parts are policy and which parts are attacker-controlled.
  </p>
  <Steps>
    <Step n={1}><b>Inputs combined.</b> System, developer, and user messages are brought together, sometimes with memory. Keep important rules in clearly marked blocks so untrusted text can’t pretend to be policy. (<Link href="https://genai.owasp.org/llmrisk/llm01-prompt-injection/" target="_blank" rel="noreferrer">OWASP</Link>)</Step>
    <Step n={2}><b>Pre-processing.</b> Retrieval and tool results are added. This is the main path for indirect injection. Remove active content, split large docs, and attach source labels. (<Link href="https://www.microsoft.com/security/blog/2024/12/03/how-to-defend-llm-agents-against-prompt-injection/" target="_blank" rel="noreferrer">Microsoft Security</Link>)</Step>
    <Step n={3}><b>Tokenization.</b> Long prompts may get cut off. Attackers can pad prompts so safety text falls out of the window. Evals show injection success rises when guardrails are pushed out. (<Link href="https://huggingface.co/blog/evaluating-prompt-injection" target="_blank" rel="noreferrer">Hugging Face</Link>)</Step>
    <Step n={4}><b>Inference.</b> Without frequent reminders of role and policy, the model treats all tokens similarly. Adding provenance tags and short restatements helps keep the model anchored. (<Link href="https://www.microsoft.com/en-us/security/blog/2024/04/11/how-microsoft-discovers-and-mitigates-evolving-attacks-against-ai-guardrails/" target="_blank" rel="noreferrer">Microsoft Spotlighting</Link>)</Step>
    <Step n={5}><b>Post-processing.</b> Formatting and tool calls can echo or act on injected text. Validate tool arguments, require citations, and log outputs for review. (<Link href="https://www.microsoft.com/security/blog/2024/12/03/how-to-defend-llm-agents-against-prompt-injection/" target="_blank" rel="noreferrer">Microsoft Security</Link>)</Step>
  </Steps>
  <p className="mt-4 text-sm text-black/70 sm:text-[13px]">Use this simple five‑step pipeline to map where your app can be influenced. The diagram below highlights common landing spots for supply‑chain instructions.</p>

```d2
direction: right
style.font: mono
style.font-size: 20

inputs: "Inputs\n(system / dev / user)"
inputs.style.font-size: 20
pre: "Pre-processing\n(RAG · tools · policy)"
pre.style.font-size: 20
tok: "Tokenization\n(context window)"
tok.style.font-size: 20
infer: "Model inference\n(attention · decoding)"
infer.style.font-size: 20
post: "Post-processing\n(tools · safety filters)"
post.style.font-size: 20

inputs -> pre
pre -> tok
tok -> infer
infer -> post
pre -> infer: "retrieval" {
  style.font-size: 20
}
tok -> post: "truncation" {
  style.stroke-dash: 5
  style.font-size: 20
}
```
</Section>

<Section title="Real-world incidents" meta="2024→2025">
  <p>
    Prompt injection has moved from academic demos to real incidents across companies and frameworks. The examples below tell the story of what happened, then point to the technical details and patches.
  </p>
  <p className="m-0 text-xs uppercase tracking-[0.18em] text-black/70 sm:text-[11px] sm:tracking-[0.28em]">High‑profile CVEs</p>
  <List>
    <Item><b>CVE-2025-32711 (Microsoft 365 Copilot — “EchoLeak”).</b> An attacker could silently inject commands that caused Copilot to leak sensitive Microsoft 365 context (emails, files, chats). This was a zero‑click issue affecting enterprise assistants. CVSS 9.3; fixed server‑side June 2025. (<Link href="https://nvd.nist.gov/vuln/detail/CVE-2025-32711" target="_blank" rel="noreferrer">NVD</Link>, <Link href="https://msrc.microsoft.com/update-guide/vulnerability/CVE-2025-32711" target="_blank" rel="noreferrer">Microsoft SUG</Link>)</Item>
    <Item><b>CVE-2024-8309 &amp; CVE-2024-7042 (LangChain — GraphCypherQA).</b> A query-answering chain accepted attacker text as trusted instructions, letting prompts manipulate graph queries. In practice, this meant database actions could be steered by injected text. Fixed in Python and JavaScript releases; CVSS 9.8. (<Link href="https://nvd.nist.gov/vuln/detail/CVE-2024-8309" target="_blank" rel="noreferrer">NVD 8309</Link>, <Link href="https://nvd.nist.gov/vuln/detail/CVE-2024-7042" target="_blank" rel="noreferrer">NVD 7042</Link>, <Link href="https://github.com/advisories/CVE-2024-8309" target="_blank" rel="noreferrer">Vendor advisory</Link>)</Item>
    <Item><b>CVE-2024-3098 (LlamaIndex — safe_eval bypass).</b> Code that was supposed to safely evaluate expressions could be bypassed via crafted input, enabling code execution paths through injected text. Patched in llama-index-core 0.10.24; CVSS 9.8. (<Link href="https://nvd.nist.gov/vuln/detail/CVE-2024-3098" target="_blank" rel="noreferrer">NVD</Link>, <Link href="https://github.com/advisories/GHSA-wvpx-g427-q9wc" target="_blank" rel="noreferrer">GitHub advisory</Link>)</Item>
    <Item><b>CVE-2024-7110 (GitLab EE — assistant actions).</b> A prompt injection path in “Resolve Vulnerability” let attacker text influence CI/CD commands. The root issue was treating untrusted content as instructions. Fixed in 17.1.6/17.2.4/17.3.1. (<Link href="https://about.gitlab.com/releases/2024/08/21/patch-release-gitlab-17-3-1-released/" target="_blank" rel="noreferrer">GitLab release notes</Link>, <Link href="https://nvd.nist.gov/vuln/detail/CVE-2024-7110" target="_blank" rel="noreferrer">NVD</Link>, <Link href="https://about.gitlab.com/releases/2024/08/21/security-release-gitlab-17-3-1-released/" target="_blank" rel="noreferrer">Vendor advisory</Link>)</Item>
  </List>

  <p>Attackers often hide instructions inside external content that the AI later ingests. When that content is treated as trusted text, it can redirect behavior or leak data.</p>
  <p className="m-0 text-xs uppercase tracking-[0.18em] text-black/70 sm:text-[11px] sm:tracking-[0.28em]">Indirect injection incidents</p>
  <List>
    <Item><b>Google Gemini calendar hijacking.</b> Poisoned calendar invites included hidden instructions that reached smart‑home actions through the assistant. Devices were controlled and data exfiltrated without obvious prompts. Google added detection and confirmations. (<Link href="https://www.wired.com/story/google-gemini-calendar-invite-hijack-smart-home" target="_blank" rel="noreferrer">Wired</Link>, <Link href="https://stavc.github.io/Web/publication/inviationallneed/" target="_blank" rel="noreferrer">Research page</Link>)</Item>
    <Item><b>Microsoft Copilot markdown exfiltration.</b> Reference‑style markdown images caused the system to auto‑fetch attacker URLs via trusted proxies, carrying data in the request. A supply‑chain flavored prompt path; patched June 2025. (<Link href="https://www.microsoft.com/en-us/msrc/blog/2025/07/how-microsoft-defends-against-indirect-prompt-injection-attacks" target="_blank" rel="noreferrer">MSRC blog</Link>)</Item>
    <Item><b>Writer.com hidden‑text leakage.</b> White‑on‑white webpage text told the assistant to exfiltrate contents using image URL parameters allowed by CSP. A simple hiding trick with real impact. Fixed Dec 18, 2023. (<Link href="https://simonwillison.net/2023/Dec/15/writercom-indirect-prompt-injection/" target="_blank" rel="noreferrer">Analysis</Link>)</Item>
    <Item><b>GPT‑4V image prompts.</b> Overlays, stretching, or steganography inside images carried commands that overrode system instructions and triggered tools. Mitigations exist, but the class persists. (<Link href="https://openai.com/index/gpt-4v-system-card/" target="_blank" rel="noreferrer">System card</Link>, <Link href="https://arxiv.org/abs/2312.01886" target="_blank" rel="noreferrer">Research</Link>)</Item>
  </List>

  <p>These risks also show up in the supply‑chain: data you index, models you download, and repos you trust. That connects directly to Door 05’s theme.</p>
  <p className="m-0 text-xs uppercase tracking-[0.18em] text-black/70 sm:text-[11px] sm:tracking-[0.28em]">RAG poisoning &amp; supply‑chain</p>
  <List>
    <Item><b>CPA‑RAG (Alibaba BaiLian).</b> Adversarial passages planted in corpora reliably steered retrieval and answers, even in a commercial platform. The attack felt like “SEO for your RAG,” but malicious. (<Link href="https://arxiv.org/abs/2505.19864" target="_blank" rel="noreferrer">arXiv</Link>)</Item>
    <Item><b>POISONCRAFT.</b> Crafted text influenced retriever ranking and citations across models, showing transfer effects and practical impact with public code. (<Link href="https://arxiv.org/abs/2505.06579" target="_blank" rel="noreferrer">arXiv</Link>, <Link href="https://www.usenix.org/conference/usenixsecurity25/presentation/zou-poisonedrag" target="_blank" rel="noreferrer">USENIX Security '25</Link>)</Item>
    <Item><b>Malicious Hugging Face models.</b> Corrupted model files and compromised Spaces secrets led to backdoors and data exposure. Download sources and conversion bots became part of the threat path. (<Link href="https://www.reversinglabs.com/press-releases/reversinglabs-identifies-novel-ml-malware-hosted-on-leading-hugging-face-ai-model-platform" target="_blank" rel="noreferrer">ReversingLabs</Link>, <Link href="https://arstechnica.com/security/2024/03/hugging-face-the-github-of-ai-hosted-code-that-backdoored-user-devices/" target="_blank" rel="noreferrer">Ars Technica</Link>)</Item>
    <Item><b>PoisonGPT.</b> A typosquatted model with surgically edited facts passed benchmarks while spreading misinformation. It looked normal but answered specific topics incorrectly. (<Link href="https://www.vice.com/en/article/researchers-demonstrate-ai-supply-chain-disinfo-attack-with-poisongpt/" target="_blank" rel="noreferrer">VICE</Link>, <Link href="https://www.elastic.co/search-labs/blog/retrieval-vs-poison-fighting-ai-supply-chain-attacks" target="_blank" rel="noreferrer">Elastic</Link>)</Item>
  </List>
</Section>

<Section title="Mitigation playbook" meta="DEFENSE" >
  <p>
    These controls live mostly in your system and infrastructure: how you structure prompts, route data, and gate which actions an agent is allowed to take.
  </p>
  <List>
    <Item><b>Separate roles + policy channels.</b> Keep system/developer instructions isolated; annotate provenance so the model can distinguish authoritative tokens. OWASP recommends explicit instruction headers and sealed “system” regions.</Item>
    <Item><b>Zero-trust untrusted content.</b> Encode or chunk external text, strip control phrases, and run heuristic/ML detectors (Prompt Shields, Spotlighting classifiers) before concatenation. (<Link href="https://www.microsoft.com/security/blog/2024/12/03/how-to-defend-llm-agents-against-prompt-injection/" target="_blank" rel="noreferrer">Microsoft Security</Link>)</Item>
    <Item><b>Constrain tools and data paths.</b> Allow-list functions, enforce parameter schemas, and add human approvals for sensitive actions. Indirect attacks often aim to trigger tool calls or unsafe connectors. (<Link href="https://genai.owasp.org/llmrisk/llm01-prompt-injection/" target="_blank" rel="noreferrer">OWASP</Link>)</Item>
    <Item><b>Continuous eval + red teaming.</b> Use suites like Hugging Face’s prompt-injection eval or OWASP incident prompts; regress known payloads in CI to keep defenses honest.</Item>
  </List>
</Section>

<Section title="Prompt Engineering Primer (for defense)" meta="OPERATIONS" >
  <p>
    This section focuses on day-to-day prompt design and operator practices that complement the system-level mitigations above.
  </p>
  <List>
    <Item><b>Roles + rules.</b> Declare operator role and numbered constraints in a dedicated System section; restate critical guardrails in developer prompts.</Item>
    <Item><b>Minimal context.</b> Provide only what’s needed for the next step; prefer cited snippets over whole documents.</Item>
    <Item><b>Output contracts.</b> Specify strict formats (JSON Schemas/regex) and rejection criteria; reject outputs lacking provenance.</Item>
    <Item><b>Examples + counterexamples.</b> Include one good + one bad example to anchor compliant vs. non-compliant behavior.</Item>
  </List>
</Section>

<Section title="Try It Yourself: Local Prompt Injection Lab" meta="TUTORIAL" >
  <p>
    A safe, local exercise to explore how prompt injection works end‑to‑end. Keep everything on an isolated machine and never test on production systems or with real data.
  </p>
  <div className="space-y-3">
    <p className="m-0 text-xs uppercase tracking-[0.18em] text-black/70 sm:text-[11px] sm:tracking-[0.28em]">Suggested stack (reference)</p>
    <List>
      <Item><b>Ollama 0.3.x.</b> Local GGUF runner + <Link href="https://ollama.com/library/llama3" target="_blank" rel="noreferrer">llama3:8b-instruct</Link> for reproducible latency.</Item>
      <Item><b>LlamaIndex or LangChain.</b> Simple RAG orchestrator to stitch memory + tools; enable per-node metadata.</Item>
      <Item><b>Chroma or Qdrant.</b> Embedding store seeded with clean policy docs plus one poisoned note.</Item>
      <Item><b>Guardrails layer.</b> Rebuff, LangSmith evals, or custom Spotlighting classifier to flag redirected intents.</Item>
      <Item><b>Observability.</b> Langfuse/LangSmith traces + OpenTelemetry exporter so you can diff prompt concatenation between runs.</Item>
    </List>
  </div>
  <Steps>
    <Step n={1}><b>Set up your environment.</b> Install Ollama, pull the <code>llama3:8b-instruct</code> model, and create a fresh project or notebook with an isolated API key and test data.</Step>
    <Step n={2}><b>Build a simple vulnerable agent.</b> Wire a basic chat loop that concatenates system prompts, user input, and retrieved documents into a single message and lets the model call at least one tool.</Step>
    <Step n={3}><b>Seed clean + poisoned content.</b> Index a few benign “policy” documents alongside one clearly marked poisoned note that contains hidden or conflicting instructions.</Step>
    <Step n={4}><b>Trigger and observe the injection.</b> Ask the agent to follow policy, then craft prompts that surface the poisoned document and watch how its instructions override the intended behavior.</Step>
    <Step n={5}><b>Add defenses from this door.</b> Refactor the agent to separate policy from data, label retrieved snippets, constrain tools, and add basic detection for suspicious instructions.</Step>
    <Step n={6}><b>Compare traces before and after.</b> Use your observability stack to diff prompts, tool calls, and outputs between the vulnerable and hardened versions, and record what changed.</Step>
  </Steps>
  <p className="text-sm text-black/70 sm:text-[13px]">Treat this lab as a sandbox for experimenting with prompt injection safely. You can expand each step with your own code, evals, and tooling as your stack evolves.</p>
</Section>
