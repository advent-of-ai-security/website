---
title: Door 09 - Misinformation
description: How LLM hallucinations, training data biases, and overreliance create credible-sounding false information that damages trust, safety, and legal compliance.
date: 2025-12-21
meta:
  - Door 09
  - OWASP - LLM09:2025
---
import Quote from '@/components/ui/Quote.tsx';
import Section from '@/components/ui/Section.tsx';
import Link from '@/components/ui/Link.tsx';
import { List, Item } from '@/components/ui/List.tsx';
import { Steps, Step } from '@/components/ui/Steps.tsx';
import MisinformationLab from '@/components/ui/MisinformationLab.tsx';

<Section title="Why You Need to Know This" meta="ATTENTION">
  <p>
    A lawyer needs precedents for an upcoming case. They ask ChatGPT for relevant court decisions and receive six perfectly formatted citations with case names, dates, and legal reasoning. The brief is submitted to the court. One problem: every single case was hallucinated – none of them exist. The lawyer is sanctioned, their client's case compromised, and "AI-generated fake citations" becomes a cautionary tale.
  </p>
  <p>
    Misinformation is the #9 risk in OWASP's 2025 LLM rankings because LLMs generate false information with the same confident, authoritative tone as accurate information. Research shows that deceptive AI-generated explanations are <i>more convincing</i> than honest explanations, and even top models exhibit hallucinations in up to 86% of generated atomic facts. Whether you're building chatbots, coding assistants, or enterprise tools, understanding why LLMs lie – and how to catch them – is essential.
  </p>
</Section>

<Section title="What You Will Learn" meta="OBJECTIVES">
  <p>By the end of this module, you will be able to:</p>
  <List>
    <Item>
      <b>Explain</b> why LLMs hallucinate and why their confident output style makes misinformation particularly dangerous.
    </Item>
    <Item>
      <b>Identify</b> misinformation types including factual hallucinations, citation fabrication, temporal confusion, and bias-induced errors.
    </Item>
    <Item>
      <b>Apply</b> defense strategies: RAG grounding, uncertainty quantification, automated fact-checking, and human-in-the-loop verification.
    </Item>
    <Item>
      <b>Evaluate</b> your LLM deployments against a checklist for grounding, citation verification, and hallucination detection.
    </Item>
  </List>
</Section>

<Section title="TL;DR" meta="AT A GLANCE">
  <div className="space-y-4">
    <Quote minimal source="OWASP LLM09:2025 Misinformation" href="https://genai.owasp.org/llmrisk/llm092025-misinformation/">
      Misinformation from LLMs poses a core vulnerability for applications relying on these models. LLMs can generate false but credible-sounding information (hallucinations), amplified by user overreliance – leading to security breaches, reputational harm, and legal liability.
    </Quote>
    <p>
      LLMs don't verify facts – they predict text patterns. When data is missing, they fabricate plausible-sounding answers with the same confident tone as accurate information. Defense requires multi-layered verification: RAG grounding, automated fact-checking, uncertainty communication, and mandatory human review for high-stakes decisions.
    </p>
  </div>
</Section>

<Section title="What Is Misinformation in LLMs?" meta="DEFINITION">
  <p>
    Misinformation in LLM context refers to <b>confident, coherent, but factually incorrect information</b> generated by the model. Unlike traditional software bugs that crash or return errors, LLMs fail gracefully – they generate plausible lies that humans accept as truth.
  </p>
  <p>
    The root cause is the model's architecture: LLMs are trained to predict the next word, not to verify facts. When asked "Who won the 2024 Super Bowl?" and lacking that data, the model doesn't say "I don't know" – it generates a statistically likely team name based on historical patterns, confidently asserting fiction.
  </p>
  <Quote source="OWASP LLM09:2025" href="https://genai.owasp.org/llmrisk/llm092025-misinformation/">
    Hallucination occurs when LLMs fill gaps in their training data using statistical patterns, resulting in answers that sound accurate but are unfounded. Biases in training data and incomplete information amplify this effect.
  </Quote>
  <p>
    Understanding the real-world impact of LLM misinformation is critical for anyone deploying these systems in production.
  </p>
</Section>

<Section title="Why It Matters" meta="IMPACT">
  <p>
    Misinformation isn't just an accuracy problem – it's a security, legal, and safety crisis when LLMs are deployed in high-stakes contexts. Recent research reveals misinformation actively changes human beliefs through persuasive deception.
  </p>
  <List>
    <Item>
      <b>Deceptive Explanations Outperform Truth.</b> <Link href="https://dl.acm.org/doi/10.1145/3706598.3713408">CHI 2025 research</Link> with 11,780 observations from 589 participants demonstrates that deceptive AI-generated explanations are <b>more convincing than honest explanations</b>, increasing belief in false news while undermining true information. Even logically invalid deceptive arguments retain persuasive power, revealing a fundamental vulnerability in human-AI interaction.
    </Item>
    <Item>
      <b>AI-Generated Fake News Epidemic.</b> <Link href="https://arxiv.org/abs/2410.19250">2024 research</Link> shows LLMs systematically generate convincing fake news at scale. While LLMs detect real news 68% better than humans, both achieve only ~60% accuracy detecting AI-generated fake content – demonstrating the asymmetric advantage attackers have in creating vs. detecting misinformation.
    </Item>
    <Item>
      <b>Security Breaches via False Guidance.</b> LLMs used in security operations might hallucinate: "CVE-2024-FAKE is patched in version 2.1" when it doesn't exist, causing teams to ignore real vulnerabilities or deploy unpatched systems.
    </Item>
    <Item>
      <b>Legal Liability (Fake Citations).</b> Lawyers using LLMs have submitted briefs citing <b>non-existent court cases</b> that the model hallucinated. Courts sanctioned attorneys for relying on AI-generated false precedents. <Link href="https://www.indusface.com/blog/owasp-top-10-llm/">Multiple documented cases</Link> in 2023-2024.
    </Item>
    <Item>
      <b>Medical Misinformation.</b> Healthcare chatbots hallucinating drug interactions, dosages, or treatment protocols create life-threatening risks. Users trust medical advice delivered in authoritative language without verifying against clinical sources.
    </Item>
    <Item>
      <b>Financial Losses.</b> Investment chatbots generating false market analysis or non-existent financial instruments lead to poor investment decisions and regulatory violations.
    </Item>
    <Item>
      <b>Code Generation Risks.</b> LLMs hallucinating API methods, library functions, or security patterns cause developers to ship vulnerable code. Example: Model invents a "safe" SQL query pattern that's actually injection-vulnerable.
    </Item>
  </List>
  <p>
    To understand how misinformation manifests, we need to examine the three factors that combine to make LLM outputs unreliable.
  </p>
</Section>

<Section title="The Misinformation Triangle" meta="FUNDAMENTALS">
  <p>
    LLM misinformation emerges from the interaction of three factors, each amplifying the others.
  </p>
  <Steps>
    <Step n={1}>
      <b>Architectural Limitation: Pattern Prediction</b>
      <p className="text-sm mt-2">
        LLMs are trained to predict the next token, not to verify truth. They have no mechanism to distinguish fact from plausible fiction – both are just patterns in text.
      </p>
      <p className="text-sm">
        <i>Result:</i> When training data is incomplete or absent, models generate statistically likely content rather than admitting uncertainty.
      </p>
      <p className="text-sm">
        <b>Example:</b> Asked about a recent event after training cutoff, the model invents a plausible answer instead of saying "I don't know."
      </p>
    </Step>
    <Step n={2}>
      <b>Output Style: Confident Authority</b>
      <p className="text-sm mt-2">
        LLMs are trained to produce fluent, confident text. False information is delivered with the same authoritative tone as verified facts – there's no linguistic signal of uncertainty.
      </p>
      <p className="text-sm">
        <i>Result:</i> Users cannot distinguish accurate from hallucinated content based on output style alone.
      </p>
      <p className="text-sm">
        <b>Example:</b> "The Supreme Court ruled in Smith v. Jones (2019) that..." sounds identical whether the case exists or not.
      </p>
    </Step>
    <Step n={3}>
      <b>Human Factor: Automation Bias</b>
      <p className="text-sm mt-2">
        Users trust automated systems more than they should, especially when outputs appear sophisticated and knowledgeable. This "automation bias" causes users to accept LLM outputs without verification.
      </p>
      <p className="text-sm">
        <i>Result:</i> Hallucinated information gets integrated into decisions, documents, and downstream systems.
      </p>
      <p className="text-sm">
        <b>Example:</b> A developer copies AI-generated code without testing because "it looks right."
      </p>
    </Step>
  </Steps>
  <p>
    With these factors in mind, let's examine the specific types of misinformation LLMs produce.
  </p>
</Section>

<Section title="Taxonomy of Misinformation" meta="ATTACK VECTORS">
  <p>
    Misinformation manifests in multiple forms, each with distinct causes and detection challenges.
  </p>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">Content Fabrication</p>
  <p className="text-sm text-neutral-400 mb-4">The model invents information that doesn't exist.</p>
  <List>
    <Item>
      <b>Factual Hallucinations.</b> The model invents facts, statistics, or events that never occurred. Example: "The Eiffel Tower was completed in 1895" (actually 1889) or "Python 4.0 introduces quantum computing primitives" (doesn't exist).
      <p className="text-xs text-neutral-500 mt-1">Detection: Medium – fact-checking tools and knowledge bases can verify.</p>
    </Item>
    <Item>
      <b>Citation Fabrication.</b> LLMs generate plausible-looking academic citations, URLs, or court cases that don't exist. These "zombie citations" appear in footnotes, confuse researchers, and mislead legal proceedings.
      <p className="text-xs text-neutral-500 mt-1">Detection: Easy – automated URL/citation verification catches most.</p>
    </Item>
    <Item>
      <b>Factual vs. Fidelity Hallucinations.</b> <Link href="https://arxiv.org/abs/2504.17550">HalluLens benchmark</Link> distinguishes between <i>factual</i> hallucinations (incorrect information) and <i>fidelity</i> hallucinations (content deviating from input context). Research shows even top models exhibit hallucinations in up to <b>86% of generated atomic facts</b> depending on domain.
      <p className="text-xs text-neutral-500 mt-1">Detection: Hard – requires semantic comparison against source material.</p>
    </Item>
  </List>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">Knowledge Gaps</p>
  <p className="text-sm text-neutral-400 mb-4">Limitations in training data or domain expertise.</p>
  <List>
    <Item>
      <b>Temporal Confusion.</b> Training data has cutoff dates. Models confidently answer questions about recent events using outdated information or fabricated updates. "Who won the 2025 election?" might return a hallucinated answer.
      <p className="text-xs text-neutral-500 mt-1">Detection: Medium – timestamp awareness and RAG grounding help.</p>
    </Item>
    <Item>
      <b>Expertise Misrepresentation.</b> Models present themselves as authorities in specialized domains where they have superficial training data, generating confident but wrong technical/medical/legal advice.
      <p className="text-xs text-neutral-500 mt-1">Detection: Hard – requires domain expert review.</p>
    </Item>
  </List>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">Systemic Issues</p>
  <p className="text-sm text-neutral-400 mb-4">Problems that compound across interactions.</p>
  <List>
    <Item>
      <b>Bias-Induced Misinformation.</b> Training data biases cause the model to present skewed or false representations as fact. Historical biases, demographic stereotypes, and cultural misrepresentations get presented authoritatively.
      <p className="text-xs text-neutral-500 mt-1">Detection: Hard – requires awareness of specific bias patterns.</p>
    </Item>
    <Item>
      <b>Compounding Errors.</b> When users iterate with LLMs, early hallucinations become "facts" in context for subsequent responses, creating cascading misinformation where each answer builds on previous false statements.
      <p className="text-xs text-neutral-500 mt-1">Detection: Very hard – requires tracking conversation history for consistency.</p>
    </Item>
  </List>

  <p className="mt-4">
    These misinformation patterns have caused significant real-world harm across multiple domains.
  </p>
</Section>

<Section title="Real-World Incidents" meta="CASE STUDIES">
  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-2 mb-3">Legal & Regulatory</p>
  <p className="text-sm text-neutral-400 mb-4">Court-documented cases of LLM misinformation.</p>
  <List>
    <Item>
      <b>Lawyer Sanctioned for AI-Generated Fake Cases (2023).</b> <Link href="https://arstechnica.com/tech-policy/2023/06/lawyer-who-used-chatgpt-to-cite-fake-cases-gets-sanctioned-by-court/">Attorney Steven Schwartz</Link> used ChatGPT for legal research, submitting a brief citing six cases – all hallucinated. The court sanctioned Schwartz for failure to verify, setting precedent for LLM misinformation liability.
    </Item>
    <Item>
      <b>Air Canada Chatbot Refund Fiasco (2024).</b> <Link href="https://arstechnica.com/tech-policy/2024/02/air-canada-must-honor-refund-policy-invented-by-airlines-chatbot/">Air Canada's chatbot</Link> hallucinated a bereavement fare policy that didn't exist. When a customer relied on this misinformation, the airline was held legally liable and ordered to honor the false policy.
    </Item>
  </List>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">Healthcare & Safety</p>
  <p className="text-sm text-neutral-400 mb-4">Life-critical misinformation in medical contexts.</p>
  <List>
    <Item>
      <b>Medical Chatbot Dosage Errors.</b> Multiple reports of health chatbots providing incorrect medication dosages, drug interactions, or contraindications – creating patient safety risks when users follow AI advice without consulting professionals.
    </Item>
  </List>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">Technical & Financial</p>
  <p className="text-sm text-neutral-400 mb-4">Developer tools and financial systems generating false information.</p>
  <List>
    <Item>
      <b>GitHub Copilot Insecure Code Generation.</b> <Link href="https://www.firetail.ai/blog/llm09-misinformation">Research shows</Link> Copilot frequently generates code with hallucinated security functions or deprecated patterns labeled as "secure," leading developers to ship vulnerable applications.
    </Item>
    <Item>
      <b>Financial Advisor Hallucinated Investments (2024).</b> An LLM-powered investment chatbot recommended specific "high-yield bonds" that didn't exist, causing users to waste time searching for non-existent financial products.
    </Item>
  </List>
</Section>

<Section title="Defense Strategy Tiers" meta="SECURITY CONTROLS">
  <p>
    Defense requires multi-layered verification, system design that acknowledges uncertainty, and user education.
  </p>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-4 mb-3">Tier 1: Essential</p>
  <p className="text-sm text-neutral-400 mb-4">Non-negotiable controls for any production LLM deployment.</p>
  <List>
    <Item>
      <b>RAG Grounding.</b> Instead of relying on parametric knowledge (training data), use Retrieval-Augmented Generation to fetch verified, current information from trusted sources. The model's response is grounded in retrieved facts, reducing hallucination rates.
    </Item>
    <Item>
      <b>Human-in-the-Loop for Critical Decisions.</b> For high-stakes domains (medical, legal, financial), require human expert review before LLM outputs are acted upon. Display clear disclaimers: "AI-generated content – verify before use."
    </Item>
    <Item>
      <b>User Education & Interface Design.</b> Design UIs that encourage skepticism: Display confidence scores, highlight when information might be outdated, and provide easy verification tools. Educate users never to rely solely on LLM outputs for critical decisions.
    </Item>
  </List>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">Tier 2: Standard</p>
  <p className="text-sm text-neutral-400 mb-4">Production-grade controls for systems making factual claims.</p>
  <List>
    <Item>
      <b>Uncertainty Quantification.</b> Implement methods to estimate model confidence. When confidence is low, explicitly state uncertainty: "I'm not certain, but based on limited data..." Use token probability analysis or multiple sampling to detect low-confidence responses.
    </Item>
    <Item>
      <b>Automatic Fact-Checking.</b> Integrate fact-checking tools: When the model makes factual claims, automatically query trusted databases (Wikipedia API, academic databases, official documentation) to verify. Flag or reject unverified claims.
    </Item>
    <Item>
      <b>Citation Verification.</b> Require the model to provide sources for factual statements. Automatically verify that cited URLs exist and contain relevant content. Reject responses with fabricated citations.
    </Item>
  </List>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">Tier 3: Advanced</p>
  <p className="text-sm text-neutral-400 mb-4">Specialized controls for high-accuracy requirements.</p>
  <List>
    <Item>
      <b>Domain Fine-Tuning.</b> Fine-tune models on curated, verified datasets specific to your domain. Reduces generic hallucinations but requires ongoing updates as domain knowledge evolves.
    </Item>
    <Item>
      <b>Hallucination Detection Classifiers.</b> Train secondary models to detect hallucinations in outputs. <Link href="https://arxiv.org/abs/2504.17550">HalluLens benchmarks</Link> check for: unsupported claims, temporal inconsistencies, fabricated entity names, or contradictions with retrieved context.
    </Item>
  </List>
</Section>

<Section title="Builder's Security Checklist" meta="IMPLEMENTATION">
  <p>Before deploying LLM applications in production, verify:</p>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-4 mb-3">For Everyone</p>
  <List>
    <Item>
      <b>Grounding Mechanism Implemented.</b> Are responses anchored to retrieved, verifiable information (RAG) rather than pure parametric generation?
    </Item>
    <Item>
      <b>Critical Disclaimers Displayed.</b> For high-stakes domains, do users see prominent warnings about verifying AI-generated content?
    </Item>
    <Item>
      <b>Human Review Required.</b> Are critical outputs (legal advice, medical recommendations, financial guidance) reviewed by qualified professionals before use?
    </Item>
  </List>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">For Developers</p>
  <List>
    <Item>
      <b>Uncertainty Communicated.</b> Does the system display confidence scores or explicitly state when information is uncertain?
    </Item>
    <Item>
      <b>Fact-Checking Automated.</b> Are factual claims automatically verified against trusted sources before display?
    </Item>
    <Item>
      <b>Citations Verified.</b> Do you validate that all cited sources exist and are relevant (no zombie citations)?
    </Item>
    <Item>
      <b>Domain Fine-Tuning Applied.</b> For specialized applications, have you fine-tuned on curated, expert-verified datasets?
    </Item>
  </List>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">For Security Teams</p>
  <List>
    <Item>
      <b>Hallucination Detection Active.</b> Do you scan outputs for common hallucination patterns (fabricated entities, temporal errors, unsupported claims)?
    </Item>
    <Item>
      <b>Output Monitoring Enabled.</b> Are you logging and reviewing LLM outputs for patterns of misinformation or user complaints about accuracy?
    </Item>
  </List>
  <p>
    The following simulation demonstrates how hallucinations appear in practice and how verification mechanisms help detect fabricated content.
  </p>
</Section>

<Section title="Interactive Simulation" meta="LAB" noPadding>
  <div className="px-6 pt-6 pb-4">
    <p className="text-sm text-neutral-300 mb-2">
      Experience how LLM hallucinations appear in practice. This simulation demonstrates how confidently-stated misinformation can mislead users, and how verification mechanisms like RAG grounding and citation checking help detect fabricated content.
    </p>
  </div>
  <MisinformationLab client:load />
</Section>

<Section title="Key Takeaways" meta="SUMMARY">
  <List>
    <Item>
      <b>Hallucinations Are Not Bugs – They're Features.</b> LLMs are designed to generate text, not verify truth. Misinformation is a fundamental limitation of current architectures, not something that will be "fixed" soon.
    </Item>
    <Item>
      <b>Confidence ≠ Correctness.</b> LLMs generate false information with the same fluent, authoritative tone as accurate information. Linguistic confidence is not epistemic reliability.
    </Item>
    <Item>
      <b>Overreliance is the Amplifier.</b> The technical problem (hallucinations) combines with the human problem (automation bias) to create risks. Users must be trained to distrust and verify AI outputs.
    </Item>
    <Item>
      <b>Legal and Safety Liability is Real.</b> Courts and regulators are holding organizations accountable for AI-generated misinformation. Deploying LLMs without verification mechanisms creates legal exposure.
    </Item>
    <Item>
      <b>Mitigation is Multi-Layered.</b> No single technique suffices. Effective defense combines RAG, fact-checking, human review, uncertainty quantification, and user education.
    </Item>
  </List>
</Section>

<Section title="Further Reading" meta="RESOURCES">
  <p className="text-sm text-neutral-300 mb-4">
    Research papers, detection methods, and guidelines for mitigating hallucinations and misinformation in LLM applications.
  </p>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-2 mb-3">Start Here</p>
  <List>
    <Item>
      <Link href="https://genai.owasp.org/llmrisk/llm092025-misinformation/">OWASP LLM09:2025 - Misinformation</Link> – Official documentation and mitigation guidelines.
    </Item>
    <Item>
      <Link href="https://www.firetail.ai/blog/llm09-misinformation">LLM09: Misinformation</Link> – Practical guide with code generation examples (FireTail).
    </Item>
  </List>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">Deep Dives</p>
  <List>
    <Item>
      <Link href="https://dl.acm.org/doi/10.1145/3706598.3713408">Deceptive AI Explanations: How Misinformation Shapes Belief</Link> – CHI 2025 research showing deceptive AI explanations are more convincing than honest ones.
    </Item>
    <Item>
      <Link href="https://arxiv.org/abs/2410.19250">Have LLMs Reopened the Pandora's Box of AI-Generated Fake News?</Link> – Research on LLM-generated misinformation and detection challenges (2024).
    </Item>
    <Item>
      <Link href="https://arxiv.org/abs/2504.17550">HalluLens: LLM Hallucination Benchmark</Link> – Framework distinguishing factual vs. fidelity hallucinations across domains.
    </Item>
    <Item>
      <Link href="https://www.indusface.com/blog/owasp-top-10-llm/">OWASP Top 10 LLM Applications 2025</Link> – Comprehensive overview of all LLM security risks (Indusface).
    </Item>
    <Item>
      <Link href="https://www.pointguardai.com/blog/understanding-the-owasp-top-10-for-llms">Understanding the 2025 OWASP Top 10 for LLM Applications</Link> – Enterprise perspective on LLM security (PointGuard AI).
    </Item>
  </List>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">OWASP & Industry References</p>
  <List>
    <Item>
      <Link href="https://www.kff.org/health-misinformation-monitor/volume-05/">AI Chatbots as Health Information Sources: Misrepresentation of Expertise</Link> – KFF Health Misinformation Monitor, healthcare chatbot risks.
    </Item>
    <Item>
      <Link href="https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know">Air Canada Chatbot Misinformation: What Travellers Should Know</Link> – BBC, Air Canada chatbot incident analysis.
    </Item>
    <Item>
      <Link href="https://www.legaldive.com/news/chatgpt-fake-legal-cases-generative-ai-hallucinations/651557/">ChatGPT Fake Legal Cases: Generative AI Hallucinations</Link> – LegalDive, legal industry hallucination cases.
    </Item>
    <Item>
      <Link href="https://towardsdatascience.com/llm-hallucinations-ec831dcd7786">Understanding LLM Hallucinations</Link> – Towards Data Science, technical hallucination explainer.
    </Item>
    <Item>
      <Link href="https://techpolicy.press/how-should-companies-communicate-the-risks-of-large-language-models-to-users/">How Should Companies Communicate the Risks of Large Language Models to Users?</Link> – Techpolicy, risk communication best practices.
    </Item>
    <Item>
      <Link href="https://www.washingtonpost.com/media/2023/01/17/cnet-ai-articles-journalism-corrections/">A news site used AI to write articles. It was a journalistic disaster</Link> – Washington Post, CNET AI journalism failures.
    </Item>
    <Item>
      <Link href="https://www.lasso.security/blog/ai-package-hallucinations">Diving Deeper into AI Package Hallucinations</Link> – Lasso Security, code generation hallucination research.
    </Item>
    <Item>
      <Link href="https://thenewstack.io/how-to-reduce-the-hallucinations-from-large-language-models/">How to Reduce the Hallucinations from Large Language Models</Link> – The New Stack, hallucination mitigation techniques.
    </Item>
    <Item>
      <Link href="https://newsletter.victordibia.com/p/practical-steps-to-reduce-hallucination">Practical Steps to Reduce Hallucination</Link> – Victor Dibia, practical mitigation guide.
    </Item>
    <Item>
      <Link href="https://www.microsoft.com/en-us/research/publication/a-framework-for-exploring-the-consequences-of-ai-mediated-enterprise-knowledge-access-and-identifying-risks-to-workers/">A Framework for Exploring the Consequences of AI-Mediated Enterprise Knowledge</Link> – Microsoft Research, enterprise AI risk framework.
    </Item>
  </List>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">Research Papers</p>
  <List>
    <Item>
      <Link href="https://arxiv.org/abs/2304.09655">How Secure is Code Generated by ChatGPT?</Link> – arXiv, code generation security analysis.
    </Item>
  </List>

  <p className="text-xs font-bold uppercase tracking-wider text-neutral-500 mt-6 mb-3">Taxonomy & Classification</p>
  <List>
    <Item>
      <Link href="https://atlas.mitre.org/techniques/AML.T0048">MITRE ATLAS AML.T0048.002 - Societal Harm</Link> – MITRE ATLAS, misinformation attack classification.
    </Item>
  </List>
</Section>
